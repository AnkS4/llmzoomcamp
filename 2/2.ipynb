{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/l13/miniforge3/envs/llmzoomcamp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from fastembed import TextEmbedding\n",
    "import numpy as np\n",
    "import pprint\n",
    "import requests\n",
    "from qdrant_client import QdrantClient, models\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Embedding the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512,)\n",
      "-0.12\n"
     ]
    }
   ],
   "source": [
    "# Initialize the text embedding model with the specified pre-trained model\n",
    "model = TextEmbedding(model_name='jinaai/jina-embeddings-v2-small-en')\n",
    "\n",
    "# Define the input query to be embedded\n",
    "query = 'I just discovered the course. Can I join now?'\n",
    "\n",
    "# Generate the embedding for the query and extract the first (and only) embedding from the list\n",
    "embedding1 = list(model.embed([query]))[0]\n",
    "\n",
    "# Print the shape of the embedding vector\n",
    "print(embedding1.shape)\n",
    "\n",
    "# Print the minimum value in the embedding vector\n",
    "print(round(np.min(embedding1), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "# Verify the normalization of the embedding to be 1.0\n",
    "print(np.linalg.norm(embedding1))\n",
    "\n",
    "# Check the cosine similarity of same embedding\n",
    "print(embedding1.dot(embedding1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Cosine similarity with another vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "doc = 'Can I still join the course after the start date?'\n",
    "\n",
    "# Generate the embedding for the query and extract the first (and only) embedding from the list\n",
    "embedding2 = list(model.embed([doc]))[0]\n",
    "\n",
    "# Check the cosine similarity between the two embeddings\n",
    "print(round(embedding1.dot(embedding2), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [{'text': \"Yes, even if you don't register, you're still eligible to submit the homeworks.\\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\",\n",
    "  'section': 'General course-related questions',\n",
    "  'question': 'Course - Can I still join the course after the start date?',\n",
    "  'course': 'data-engineering-zoomcamp'},\n",
    " {'text': 'Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.',\n",
    "  'section': 'General course-related questions',\n",
    "  'question': 'Course - Can I follow the course after it finishes?',\n",
    "  'course': 'data-engineering-zoomcamp'},\n",
    " {'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
    "  'section': 'General course-related questions',\n",
    "  'question': 'Course - When will the course start?',\n",
    "  'course': 'data-engineering-zoomcamp'},\n",
    " {'text': 'You can start by installing and setting up all the dependencies and requirements:\\nGoogle cloud account\\nGoogle Cloud SDK\\nPython 3 (installed with Anaconda)\\nTerraform\\nGit\\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.',\n",
    "  'section': 'General course-related questions',\n",
    "  'question': 'Course - What can I do before the course starts?',\n",
    "  'course': 'data-engineering-zoomcamp'},\n",
    " {'text': 'Star the repo! Share it with friends if you find it useful ❣️\\nCreate a PR if you see you can improve the text or the structure of the repository.',\n",
    "  'section': 'General course-related questions',\n",
    "  'question': 'How can we contribute to the course?',\n",
    "  'course': 'data-engineering-zoomcamp'}]\n",
    "\n",
    "# Generate the embedding for the documents and extract the first (and only) embedding from the list\n",
    "embeddings3 = list(model.embed([doc['text'] for doc in documents]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Ranking by cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index of the maximum cosine similarity is:  1\n",
      "\n",
      "The maximum cosine similarity is:  0.82\n",
      "\n",
      "The most similar document is:  Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\n",
      "You can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.\n"
     ]
    }
   ],
   "source": [
    "# Put the embeddings3 calculated in for each document to a vector V\n",
    "V1 = np.array([embedding for embedding in embeddings3])\n",
    "\n",
    "# Calculate the cosine similarity between each embedding and the query\n",
    "cosine_similarities1 = V1.dot(embedding1)\n",
    "\n",
    "# Find the index of the maximum cosine similarity\n",
    "max_index1 = np.argmax(cosine_similarities1)\n",
    "\n",
    "# Print the document with the maximum cosine similarity\n",
    "print(\"The index of the maximum cosine similarity is: \", max_index1)\n",
    "print(\"\\nThe maximum cosine similarity is: \", round(cosine_similarities1[max_index1], 2))\n",
    "print(\"\\nThe most similar document is: \", documents[max_index1]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index of the maximum cosine similarity is:  0\n",
      "\n",
      "The maximum cosine similarity is:  0.85\n",
      "\n",
      "The most similar document is:  Yes, even if you don't register, you're still eligible to submit the homeworks.\n",
      "Be aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\n"
     ]
    }
   ],
   "source": [
    "# For each doc in documents, concatenate the question and text into a single string\n",
    "full_text = [doc['question'] + ' ' + doc['text'] for doc in documents]\n",
    "\n",
    "# Generate the embedding for the full text and extract the first (and only) embedding from the list\n",
    "embeddings4 = list(model.embed(full_text))\n",
    "\n",
    "# Put the embeddings4 calculated in for each document to a vector V\n",
    "V2 = np.array([embedding for embedding in embeddings4])\n",
    "\n",
    "# Calculate the cosine similarity between each embedding and the query\n",
    "cosine_similarities2 = V2.dot(embedding1)\n",
    "\n",
    "# Find the index of the maximum cosine similarity\n",
    "max_index2 = np.argmax(cosine_similarities2)\n",
    "\n",
    "# Print the document with the maximum cosine similarity\n",
    "print(\"The index of the maximum cosine similarity is: \", max_index2)\n",
    "print(\"\\nThe maximum cosine similarity is: \", round(cosine_similarities2[max_index2], 2))\n",
    "print(\"\\nThe most similar document is: \", documents[max_index2]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Selecting the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'additional_files': [],\n",
      " 'description': 'Text embeddings, Unimodal (text), English, 512 input tokens '\n",
      "                'truncation, Prefixes for queries/documents: necessary, 2023 '\n",
      "                'year.',\n",
      " 'dim': 768,\n",
      " 'license': 'mit',\n",
      " 'model': 'BAAI/bge-base-en',\n",
      " 'model_file': 'model_optimized.onnx',\n",
      " 'size_in_GB': 0.42,\n",
      " 'sources': {'_deprecated_tar_struct': True,\n",
      "             'hf': 'Qdrant/fast-bge-base-en',\n",
      "             'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-base-en.tar.gz'},\n",
      " 'tasks': {}}\n"
     ]
    }
   ],
   "source": [
    "# List all available text embedding models\n",
    "available_models = TextEmbedding.list_supported_models()\n",
    "\n",
    "# Check the parameters of the first model\n",
    "pprint.pprint(available_models[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The smallest dimension is 384\n",
      "\n",
      "Models with the smallest dimension:\n",
      "BAAI/bge-small-en\n",
      "BAAI/bge-small-en-v1.5\n",
      "snowflake/snowflake-arctic-embed-xs\n",
      "snowflake/snowflake-arctic-embed-s\n",
      "sentence-transformers/all-MiniLM-L6-v2\n",
      "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    }
   ],
   "source": [
    "# Find the smallest dimension\n",
    "smallest_dim = min(model['dim'] for model in available_models)\n",
    "\n",
    "# List all models with that dimension\n",
    "smallest_models = [model['model'] for model in available_models if model['dim'] == smallest_dim]\n",
    "\n",
    "print(f\"The smallest dimension is {smallest_dim}\")\n",
    "\n",
    "print(\"\\nModels with the smallest dimension:\")\n",
    "for m in smallest_models:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Indexing with qdrant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start a Qdrant vector database instance using Docker.\n",
    "\n",
    "`docker pull qdrant/qdrant`\n",
    "\n",
    "`docker run -p 6333:6333 -p 6334:6334 -v \"$(pwd)/qdrant_storage:/qdrant/storage:z\" qdrant/qdrant`\n",
    "\n",
    "Parameters:\n",
    " - `6333`: Used for HTTP API\n",
    " - `6334`: Used for gRPC API\n",
    " - `-v \"$(pwd)/qdrant_storage:/qdrant/storage:z\"`: Mounts the `qdrant_storage` directory from your current working directory to the container's `/qdrant/storage` directory, allowing data persistence, the `:z` option is for SELinux compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding documents to qdrant:   9%|▉         | 33/375 [00:02<00:20, 16.86it/s]"
     ]
    }
   ],
   "source": [
    "# Fetching new documents\n",
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "    if course_name != 'machine-learning-zoomcamp':\n",
    "        continue\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)\n",
    "\n",
    "# Check the number of documents\n",
    "print(f\"Number of documents: {len(documents)}\")\n",
    "\n",
    "# Check the first document\n",
    "# print(\"\\nFirst document:\")\n",
    "# pprint.pprint(documents[0])\n",
    "\n",
    "collection_name = \"zoomcamp-faq\"\n",
    "model_small=\"BAAI/bge-small-en\"\n",
    "\n",
    "# Create a client to connect to the Qdrant server\n",
    "client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "# Delete the collection\n",
    "# client.delete_collection(collection_name=collection_name)\n",
    "\n",
    "# Create a collection\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=384,\n",
    "        distance=models.Distance.COSINE\n",
    "        ),\n",
    ")\n",
    "\n",
    "client.create_payload_index(\n",
    "    collection_name=collection_name,\n",
    "    field_name=\"course\",\n",
    "    field_schema=\"keyword\",\n",
    ")\n",
    "\n",
    "# Add both question and answer fields\n",
    "points = []\n",
    "\n",
    "for i, doc in enumerate(documents):\n",
    "    text = doc['question'] + ' ' + doc['text']\n",
    "    vector = models.Document(text=text, model=model_small)\n",
    "    point = models.PointStruct(\n",
    "        id=i,\n",
    "        vector=vector,\n",
    "        payload=doc\n",
    "    )\n",
    "    points.append(point)\n",
    "\n",
    "# Add the documents to the collection along with progress bar\n",
    "for point in tqdm(points, desc=\"Adding documents to qdrant\"):\n",
    "    client.upsert(\n",
    "        collection_name=collection_name,\n",
    "        points=[point]\n",
    "    )\n",
    " \n",
    "# client.upsert(\n",
    "#     collection_name=collection_name,\n",
    "#     points=points\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest score:  0.87\n",
      "\n",
      "Query:  I just discovered the course. Can I join now?\n",
      "\n",
      "Answer:  Yes, you can. You won’t be able to submit some of the homeworks, but you can still take part in the course.\n",
      "In order to get a certificate, you need to submit 2 out of 3 course projects and review 3 peers’ Projects by the deadline. It means that if you join the course at the end of November and manage to work on two projects, you will still be eligible for a certificate.\n"
     ]
    }
   ],
   "source": [
    "query_points = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=models.Document(\n",
    "            text=query,\n",
    "            model=model_small \n",
    "        ),\n",
    "        limit=1,\n",
    "        with_payload=True\n",
    "    )\n",
    "    \n",
    "print(\"Highest score: \", round(query_points.points[0].score, 2))\n",
    "print(\"\\nQuery: \", query)\n",
    "print(\"\\nAnswer: \", query_points.points[0].payload['text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmzoomcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
